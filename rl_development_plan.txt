# Ember Ranger - Reinforcement Learning Development Plan

This document outlines the development tasks for the reinforcement learning components of the Ember Ranger project, based on the provided project proposal.

## Phase 1: Simulation Environment Setup (`drone_env.py`)

The goal of this phase is to create a robust and realistic simulation environment that serves as the training ground for the RL agent.

- **Task 1.1: DEM Data Handling**
  - **Description:** Implement a module (`drone_dem_loader.py`) to load and parse the 1m-resolution Digital Elevation Model (DEM) data. This module should provide an easy-to-use interface for the environment to access elevation at any (x, y) coordinate.
  - **Files:** `drone_dem_loader.py`, `utils.py`

- **Task 1.2: Core Environment Implementation**
  - **Description:** Create the main `DroneEnv` class structure, inheriting from a standard RL environment interface like `gym.Env`. Implement the basic `__init__`, `reset`, `step`, and `render` methods.
  - **Files:** `drone_env.py`

- **Task 1.3: State and Action Spaces**
  - **Description:** Define and implement the precise `observation_space` and `action_space` as specified in the document. 
    - **Observation Space:** (Full DEM, Explored Mask, Drone Position, Velocity, Attitude, Energy Ratio)
    - **Action Space:** (3D Acceleration Vector)
  - **Files:** `drone_env.py`

- **Task 1.4: Drone Dynamics & Physics**
  - **Description:** Implement the physics of the drone's movement within the `step` function. This involves updating the drone's position, velocity, and attitude based on the chosen acceleration (action), gravity, and air resistance.
  - **Files:** `drone_env.py`

- **Task 1.5: Viewshed Calculation for Reward**
  - **Description:** Integrate the `calculate_view_area` function. In each `step`, calculate the newly explored surface area based on the drone's current position and attitude. This will be a key component of the reward.
  - **Files:** `drone_env.py`, `calculate_area.py`

- **Task 1.6: Comprehensive Reward Function**
  - **Description:** Implement the full reward function as a combination of all specified components: area reward, energy penalty, height penalty, distance penalty, arrival bonus, and battery-out penalty. Make the weights of each component configurable.
  - **Files:** `drone_env.py`

- **Task 1.7: Episode Termination Logic**
  - **Description:** Implement the conditions that end an episode: mission completion (>90% explored and returned to start), battery depletion, or a time limit.
  - **Files:** `drone_env.py`

## Phase 2: PPO Agent Implementation (`drone_agent.py`)

This phase focuses on building the PPO agent that will learn to control the drone.

- **Task 2.1: PPO Network Architecture**
  - **Description:** Define the neural network models for the Actor (policy) and Critic (value function). The networks must be compatible with the defined observation space.
  - **Files:** `drone_agent.py`

- **Task 2.2: Core PPO Algorithm**
  - **Description:** Implement the PPO learning logic. This includes collecting experience from the environment, calculating advantages, and performing clipped policy updates over several epochs.
  - **Files:** `drone_agent.py`

## Phase 3: Training and Evaluation

This phase involves training the agent and evaluating its performance.

- **Task 3.1: Training Loop**
  - **Description:** Create the main training script (`train.py`). This script will initialize the environment and the agent, and run the main training loop for a specified number of episodes, saving model checkpoints periodically.
  - **Files:** `train.py`, `main.py`

- **Task 3.2: Ember Detection Simulation**
  - **Description:** In the environment, add a simplified mechanism to simulate ember hotspots (e.g., specific coordinates with high temperature). The `step` function should return whether a fire was detected in its `info` dictionary to test the full mission logic.
  - **Files:** `drone_env.py`

- **Task 3.3: Evaluation Script**
  - **Description:** Create an `evaluate.py` script that loads a trained agent, runs it in the environment (with rendering/visualization), and logs key performance metrics (e.g., total explored area, time to completion, final reward).
  - **Files:** `evaluate.py`

- **Task 3.4: Sim-to-Real (Domain Randomization)**
  - **Description:** Enhance the environment by adding domain randomization features. This includes applying small, random forces (to simulate wind) or adding noise to the drone's state observations during training.
  - **Files:** `drone_env.py`
