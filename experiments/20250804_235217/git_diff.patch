diff --git a/.DS_Store b/.DS_Store
index d1705dd..70b34d0 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc b/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc
index 3ded294..8892152 100644
Binary files a/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc and b/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc differ
diff --git a/fly_drone/envs/fly_drone_env.py b/fly_drone/envs/fly_drone_env.py
index 65af3c0..f99e1ba 100644
--- a/fly_drone/envs/fly_drone_env.py
+++ b/fly_drone/envs/fly_drone_env.py
@@ -374,7 +374,7 @@ class Fly_drone(gym.Env):
     #"w_area": 2.560835061994321,
     #"w_alt": 1.5048324796298398,
     #"w_energy": 0.005251185724086418,
-    def __init__(self, log_dir: Path, plot_dir: Path, w_area : float = 2.560835061994321, w_alt : float = 1.5048324796298398, w_energy: float = 0.005251185724086418, **kwargs):
+    def __init__(self, log_dir: Path, plot_dir: Path, w_area : float = 0.1, w_alt : float = 0.05, w_energy: float = 1e-4, **kwargs):
         self.action_space = spaces.Box(low=-3.0, high=3.0, shape=(4, ), dtype="float32") #set action space size, range
         self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9 + (self.MAX_VERTICES + 1) * 2 + 1,), dtype="float32") #set observation space size, range
         self.done = False
@@ -610,11 +610,11 @@ class Fly_drone(gym.Env):
         # --------- 6) 고도 유지 보상 ---------
         target_altitude = ground_alt + 10
         altitude_error = abs(drone_alt - target_altitude)
-        altitude_reward = -0.1 * altitude_error
+        altitude_reward = -0.02 * altitude_error
         if drone_alt < ground_alt:
             altitude_reward -= 1000
         elif drone_alt > ground_alt + 20:
-            altitude_reward -= 500
+            altitude_reward -= 20
         altitude_reward *= self.w_alt
 
         # --------- 7) 에너지 패널티 계산 ---------
@@ -654,8 +654,17 @@ class Fly_drone(gym.Env):
 
         # 100번마다 로그 나옴
         if self._step_idx % 100 == 0:      # 100스텝마다
-            print(f"[env] ep{self.episode} step{self._step_idx}"
-                f"  t={time:.1f}s  reward={reward:.2f}")
+            #print(f"[env] ep{self.episode} step{self._step_idx}"
+            #    f"  t={time:.1f}s  reward={reward:.2f}")
+            v_xy = np.linalg.norm(drone_xy_velocity)
+            print((
+                f"[env] ep{self.episode:04d}  step{self._step_idx:05d}  t={time:6.1f}s\n"
+                f"       ▸ area_r={area_reward:8.2f}   alt_r={altitude_reward:8.2f}   "
+                f"eng_r={penalty_E:8.2f}   total_r={reward:8.2f}\n"
+                f"       ▸ explored={explored_area:8.1f} m²   "
+                f"pos=({drone_xy[0]:.1f}, {drone_xy[1]:.1f}, {drone_alt:.1f})   "
+                f"v_xy={v_xy:.2f} m/s   v_z={drone_z_velocity:.2f} m/s"
+            ))
         # ---------------------------------------------------------
 
         return state, reward, self.done, info
diff --git a/train.py b/train.py
index d5e0df0..d11ee74 100644
--- a/train.py
+++ b/train.py
@@ -20,15 +20,15 @@ import yaml
 cfg = dict(
     algo="PPO",
     env_name="FlyDrone-v0",
-    total_timesteps=2_000_0000,#실행시 2_000_000, 테스트시 10_000,
+    total_timesteps=10_000,#실행시 2_000_000, 테스트시 10_000,
     log_every_steps=20,
     checkpoint_freq=10_000,
-    #lr=3e-4,
-    lr=0.00013296321722176043,
-    #n_steps=2048,
-    n_steps=1024,
-    #gamma=0.99,
-    gamma=0.9541215081998161,
+    lr=3e-4,
+    #lr=0.00013296321722176043,
+    n_steps=2048,
+    #n_steps=1024,
+    gamma=0.995,
+    #gamma=0.9541215081998161,
     etc="…"
 )
 
@@ -199,7 +199,6 @@ reward_plot_cb = RewardPlotCallback(PLOT_DIR, plot_every_episodes=20, verbose=1)
 
 print('> 학습 곧 시작')
 model.learn(total_timesteps=cfg["total_timesteps"], callback=[checkpoint_cb, reward_plot_cb, progress_cb])
-print('> 학습 시작')
 
 model.save(MODEL_DIR / "final_model")
 vec_env.save(MODEL_DIR / "vec_normalize.pkl")