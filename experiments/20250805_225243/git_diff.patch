diff --git a/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc b/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc
index 37804d5..0f4ebfb 100644
Binary files a/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc and b/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc differ
diff --git a/fly_drone/envs/fly_drone_env.py b/fly_drone/envs/fly_drone_env.py
index ba94101..06aa5b5 100644
--- a/fly_drone/envs/fly_drone_env.py
+++ b/fly_drone/envs/fly_drone_env.py
@@ -375,8 +375,8 @@ class Fly_drone(gym.Env):
     #"w_alt": 1.5048324796298398,
     #"w_energy": 0.005251185724086418,
     def __init__(self, log_dir: Path, plot_dir: Path, w_area : float = 0.02, w_alt : float = 0.5, w_energy: float = 0.1, **kwargs):
-        self.action_space = spaces.Box(low=-3.0, high=3.0, shape=(4, ), dtype="float32") #set action space size, range
-        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9+1+1,), dtype="float32") #set observation space size, range
+        self.action_space = spaces.Box(low=-3.0, high=3.0, shape=(2, ), dtype="float32") #set action space size, range
+        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9,), dtype="float32") #set observation space size, range
         self.done = False
         self.episode = 0
         self.train = True
@@ -429,7 +429,7 @@ class Fly_drone(gym.Env):
         if v_xy > self.max_xy_speed:
             drone_xy_velocity *= (self.max_xy_speed / (v_xy + 1e-8))
         # Z 속도 제한 (스칼라이므로 clip)
-        drone_z_velocity = float(np.clip(drone_z_velocity, -self.max_z_speed, self.max_z_speed))
+        #drone_z_velocity = float(np.clip(drone_z_velocity, -self.max_z_speed, self.max_z_speed))
 
     def seed(self, seed: Optional[int] = None): 
         #옵튜나 때문에 만든 함수임
@@ -439,25 +439,12 @@ class Fly_drone(gym.Env):
         return [seed]
     
     def _build_state(self) -> np.ndarray:
-        """
-        x, y, z, vx, vy, vz, roll, pitch, yaw, explored_area 그리고
-        target_area 폴리곤 꼭짓점들을 모두 합쳐서 반환합니다.
-        """
-        px, py = world_to_pixel(np.array([drone_xy[0]]), np.array([drone_xy[1]]))
-
-        if not (0 <= px < width and 0 <= py < height):
-            # out of bounds -> 끝내고 큰 페널티
-            ground_alt = -1e6
-        else:
-            ground_alt = dem[py[0], px[0]]
-        alt_error  = drone_alt - (ground_alt + 10.0)    
         # 1) 위치·속도·자세·면적
         state = [
-            drone_xy[0], drone_xy[1], drone_alt,
+            drone_xy[0], drone_xy[1],
             drone_xy_velocity[0], drone_xy_velocity[1], drone_z_velocity,
             roll, pitch, yaw,
             explored_area,
-            alt_error
         ]
         #ground = dem(self.pos[0], self.pos[1])
         #alt_error = drone_alt - (ground + 10.0)
@@ -473,37 +460,37 @@ class Fly_drone(gym.Env):
         global roll, pitch, yaw, explored_area, all_polygons, drone_path
 
         # --------- 1) 액션 파싱 & yaw=0 강제 ---------
-        ax, ay, az, _ = action.astype(np.float32)
+        ax, ay = action.astype(np.float32)
         yaw = 0.0  # yaw는 쓰지 않으므로 0 고정  ### <--
 
+        # --------- 3) DEM/ground alt ---------
+        px, py = world_to_pixel(np.array([drone_xy[0]]), np.array([drone_xy[1]]))
+        px, py = px[0], py[0]
+
+        if not (0 <= px < width and 0 <= py < height):
+            # out of bounds -> 끝내고 큰 페널티
+            reward = -1000.0
+            self.total_return += reward
+            self.done = True
+            #return self._build_state(), reward, self.done, {}
+            return self._build_state(), reward, self.done, {}
+
         # --------- 2) 상태 적분 ---------
         time += DT
         self._step_idx += 1
+
         # pos
         drone_xy[0] += ax * 0.5 * DT**2 + drone_xy_velocity[0] * DT
         drone_xy[1] += ay * 0.5 * DT**2 + drone_xy_velocity[1] * DT
-        drone_alt   += az * 0.5 * DT**2 + drone_z_velocity      * DT
+        drone_alt   = dem[py, px] + 10
         # vel
         drone_xy_velocity[0] += ax * DT
         drone_xy_velocity[1] += ay * DT
-        drone_z_velocity     += az * DT
+        drone_z_velocity = 0.0
         self._limit_speed()
 
 
-        # --------- 3) DEM/ground alt ---------
-        px, py = world_to_pixel(np.array([drone_xy[0]]), np.array([drone_xy[1]]))
-        px, py = px[0], py[0]
 
-        if not (0 <= px < width and 0 <= py < height):
-            # out of bounds -> 끝내고 큰 페널티
-            reward = -1000.0
-            self.total_return += reward
-            self.done = True
-            #return self._build_state(), reward, self.done, {}
-            return self._build_state(), reward, self.done, {}
-    
-        ground_alt = dem[py, px]
-        alt_error  = drone_alt - (ground_alt + 10.0)
 
         '''
         # --------- 4) 시야 레이캐스팅 (기존) ---------
@@ -626,7 +613,7 @@ class Fly_drone(gym.Env):
 
         area_reward *= self.w_area
 
-
+        '''
         # --------- 6) 고도 유지 보상 ---------
         target_altitude = ground_alt + 10
         altitude_error = abs(drone_alt - target_altitude)
@@ -637,12 +624,13 @@ class Fly_drone(gym.Env):
         #altitude_reward = -0.05 * (alt_diff ** 2)            
         altitude_reward = -max(0.0, np.exp(alt_diff* 0.05))
         altitude_reward *= self.w_alt
+        '''
 
         # --------- 7) 에너지 패널티 계산 ---------
         #   - 여기서 Mx, My를 "원하는 선형가속도"로부터 만들고,
         #   - yaw=0을 유지한다고 가정.
-        acc_world = np.array([ax, ay, az], dtype=np.float64)
-        vel_world = np.array([drone_xy_velocity[0], drone_xy_velocity[1], drone_z_velocity], dtype=np.float64)
+        acc_world = np.array([ax, ay, 0.0], dtype=np.float64) # 목표 az는 0으로 고정
+        vel_world = np.array([drone_xy_velocity[0], drone_xy_velocity[1], 0.0], dtype=np.float64)
 
         Mx, My = attitude_torque_from_acc(acc_world, roll, pitch, quad_params, kp=ATT_KP)  #  <-- 핵심
         penalty_E, omegas, powers, info_E = energy_penalty_from_action(
@@ -651,15 +639,15 @@ class Fly_drone(gym.Env):
         penalty_E  *= self.w_energy
 
         # 전체 리워드
-        reward = area_reward + altitude_reward + penalty_E
+        reward = area_reward  + penalty_E
         self.total_return += reward
 
         # --------- 8) 종료 조건 ---------
         px, py = world_to_pixel(np.array([drone_xy[0]]), np.array([drone_xy[1]]))
         px, py = px[0], py[0]
+        ground_alt = dem[py, px]
+        drone_alt = ground_alt + 10.0  # 지형 위 10m
             
-        if (self.idle_counter >= 50): # idle이 50스텝 이상이면 끝
-            self.done = True
         self._check_done(ground_alt)
 
         if self.done:
@@ -697,10 +685,10 @@ class Fly_drone(gym.Env):
             print(f"[{self.episode:04d}|{self._step_idx:05d}]"
                 f" Δarea={delta_area:6.1f}  explored={explored_area:8.1f}"
                 f" | r_area={area_reward:+6.2f}"
-                f" r_alt={altitude_reward:+6.2f}"
+                '''f" r_alt={altitude_reward:+6.2f}"'''
                 f" r_E={penalty_E:+6.2f}"
                 f" | z={drone_alt:7.1f}"
-                f" err={altitude_error:5.1f}"
+                '''f" err={altitude_error:5.1f}"'''
                 f" | v_xy={v_xy:.2f} m/s   v_z={drone_z_velocity:.2f} m/s"
                 )
                 
@@ -721,7 +709,7 @@ class Fly_drone(gym.Env):
         self._prev_area = 0.0
         drone_xy = np.array([264300.0, 309370.0])
         drone_xy_velocity = np.array([0.0, 0.0])
-        drone_z_velocity = 0.0
+        #drone_z_velocity = 0.0
         roll, pitch, yaw, explored_area = 0, 0, 0, 0
         dem_minx, dem_miny, dem_maxx, dem_maxy = bounds
 
@@ -730,7 +718,7 @@ class Fly_drone(gym.Env):
         px, py = world_to_pixel(np.array([drone_xy[0]]), np.array([drone_xy[1]]))
         ground_alt = dem[py[0], px[0]]
         drone_alt = ground_alt + 10 #리셋 높이
-        alt_error  = drone_alt - (ground_alt + 10.0)
+        #alt_error  = drone_alt - (ground_alt + 10.0)
 
 
         # ---- Create a random *valid* target area with fixed number of vertices ----
@@ -759,7 +747,7 @@ class Fly_drone(gym.Env):
         state1 = [drone_xy[i] for i in range(2)]
         state2 = [drone_xy_velocity[i] for i in range(2)]
         state3 = [roll, pitch, yaw, drone_z_velocity, explored_area]
-        state = state1 + [drone_alt] + state2 + state3 + [alt_error]
+        state = state1 + state2 + state3 
 
         return state
     
@@ -768,10 +756,12 @@ class Fly_drone(gym.Env):
         if time >= 2000:
             self.done = True
 
+        '''
         # 2. Collision with ground
         if drone_alt <= ground_alt:
             self.done = True
             self.total_return -= 50 # Large penalty for collision
+        '''
 
     def settings(self, rend, train):
         self.train = train