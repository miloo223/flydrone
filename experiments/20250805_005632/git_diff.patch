diff --git a/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc b/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc
index 4fe2149..d09e3b0 100644
Binary files a/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc and b/fly_drone/envs/__pycache__/fly_drone_env.cpython-312.pyc differ
diff --git a/fly_drone/envs/fly_drone_env.py b/fly_drone/envs/fly_drone_env.py
index 4969d1c..8b68473 100644
--- a/fly_drone/envs/fly_drone_env.py
+++ b/fly_drone/envs/fly_drone_env.py
@@ -374,16 +374,16 @@ class Fly_drone(gym.Env):
     #"w_area": 2.560835061994321,
     #"w_alt": 1.5048324796298398,
     #"w_energy": 0.005251185724086418,
-    def __init__(self, log_dir: Path, plot_dir: Path, w_area : float = 0.1, w_alt : float = 0.05, w_energy: float = 1e-4, **kwargs):
+    def __init__(self, log_dir: Path, plot_dir: Path, w_area : float = 0.02, w_alt : float = 0.5, w_energy: float = 0.005, **kwargs):
         self.action_space = spaces.Box(low=-3.0, high=3.0, shape=(4, ), dtype="float32") #set action space size, range
-        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9 + (self.MAX_VERTICES + 1) * 2 + 1,), dtype="float32") #set observation space size, range
+        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9 + 1,), dtype="float32") #set observation space size, range
         self.done = False
         self.episode = 0
         self.train = True
         self.rend = True
         self.total_return = 0
         self.score_avg = 0
-        self.target_area = None
+        #self.target_area = None
         self.POLY_ENLARGE = 3.0
         self.max_xy_speed = 8.0   # ìˆ˜í‰(xy) ìµœëŒ€ ì†ë„ [m/s]
         self.max_z_speed = 5.0     # ìˆ˜ì§(z) ìµœëŒ€ ì†ë„ [m/s]
@@ -451,8 +451,8 @@ class Fly_drone(gym.Env):
             explored_area
         ]
             # 2) íƒ€ê¹ƒ í´ë¦¬ê³¤ ê¼­ì§“ì 
-        verts = as_fixed_length_coords(self.target_area, self.MAX_VERTICES)  # (N+1, 2) ë°°ì—´
-        state.extend(verts.flatten().tolist())
+        #verts = as_fixed_length_coords(self.target_area, self.MAX_VERTICES)  # (N+1, 2) ë°°ì—´
+        #state.extend(verts.flatten().tolist())
 
         return np.array(state, dtype=np.float32)
     
@@ -567,8 +567,7 @@ class Fly_drone(gym.Env):
         state1 = [drone_xy[i] for i in range(2)]
         state2 = [drone_xy_velocity[i] for i in range(2)]
         state3 = [roll, pitch, yaw, drone_z_velocity, explored_area]
-        target_vertices = np.array(self.target_area.exterior.coords)
-        state = state1 + state2 + state3 + [drone_alt] + list(target_vertices.flatten())
+        state = state1 + state2 + state3 + [drone_alt]
 
 
         # --------- 5) ë©´ì  ë³´ìƒ (patched & robust) ---------
@@ -583,6 +582,7 @@ class Fly_drone(gym.Env):
                 self.MAX_VERTICES
             )
 
+            '''
             # 3) target_areaì™€ êµì°¨í•˜ëŠ” ë¶€ë¶„ë§Œ ê³„ì‚°
             if  self.target_area is not None:
                 try:
@@ -594,7 +594,8 @@ class Fly_drone(gym.Env):
                         fix_polygon(self.target_area.buffer(0))
                     )
                     effective_poly = fix_polygon(effective_poly)
-
+            '''
+            effective_poly = fix_polygon(hull_fixed)
             if not effective_poly.is_empty:
                 if all_polygons:
                     total_area_poly = unary_union(all_polygons)
@@ -615,11 +616,14 @@ class Fly_drone(gym.Env):
 
 
         # --------- 6) ê³ ë„ ìœ ì§€ ë³´ìƒ ---------
-        target_altitude = ground_alt + 10
-        altitude_error = abs(drone_alt - target_altitude)
-        altitude_reward = -0.02 * altitude_error
-        if drone_alt < ground_alt:
-            altitude_reward -= 1000
+        #target_altitude = ground_alt + 10
+        #altitude_error = abs(drone_alt - target_altitude)
+        #altitude_reward = -0.02 * altitude_error
+        #if drone_alt < ground_alt:
+        #    altitude_reward -= 1000
+        alt_diff = max(0.0, drone_alt - (ground_alt + 10.0))
+        altitude_reward = -0.05 * (alt_diff ** 2)            
+
         altitude_reward *= self.w_alt
 
         # --------- 7) ì—ë„ˆì§€ íŒ¨ë„í‹° ê³„ì‚° ---------
@@ -698,6 +702,7 @@ class Fly_drone(gym.Env):
         self.idle_counter = 0
         self.done = False
         time = 0
+        self._prev_area = 0.0
         drone_xy = np.array([264300.0, 309370.0])
         drone_xy_velocity = np.array([0.0, 0.0])
         drone_z_velocity = 0.0
@@ -711,14 +716,13 @@ class Fly_drone(gym.Env):
         drone_alt = ground_alt + 10 #ë¦¬ì…‹ ë†’ì´
 
         # ---- Create a random *valid* target area with fixed number of vertices ----
-        center_x = np.clip(drone_xy[0] + np.random.uniform(-150, 150), dem_minx + 100, dem_maxx - 100)
-        center_y = np.clip(drone_xy[1] + np.random.uniform(-150, 150), dem_miny + 100, dem_maxy - 100)
-
+        #center_x = np.clip(drone_xy[0] + np.random.uniform(-150, 150), dem_minx + 100, dem_maxx - 100)
+        #center_y = np.clip(drone_xy[1] + np.random.uniform(-150, 150), dem_miny + 100, dem_maxy - 100)
 
-        num_points = self.MAX_VERTICES
-        # â€œì¡°ê¸ˆ ë” í° ë²”ìœ„â€ -> ë°˜ì§€ë¦„ ë²”ìœ„ë¥¼ í‚¤ì›€
-        radius = np.random.uniform(100, 200)
 
+        #num_points = self.MAX_VERTICES
+        #radius = np.random.uniform(100, 200)
+        '''
         angles = np.sort(np.random.uniform(0, 2 * np.pi, num_points))
         points = []
         for angle in angles:
@@ -732,14 +736,12 @@ class Fly_drone(gym.Env):
             fix_polygon(raw_target).buffer(self.POLY_ENLARGE),
             self.MAX_VERTICES
         )
-
+        '''
 
         state1 = [drone_xy[i] for i in range(2)]
         state2 = [drone_xy_velocity[i] for i in range(2)]
         state3 = [roll, pitch, yaw, drone_z_velocity, explored_area]
-
-        target_vertices = as_fixed_length_coords(self.target_area, self.MAX_VERTICES)
-        state = state1 + state2 + state3 + [drone_alt] + list(target_vertices.flatten())
+        state = state1 + state2 + state3 + [drone_alt]
 
         return state
     
@@ -766,11 +768,6 @@ class Fly_drone(gym.Env):
             fig, ax = plt.subplots(figsize=(16, 12), dpi=400)
             ax.imshow(dem, cmap='terrain', extent=(bounds.left, bounds.right, bounds.bottom, bounds.top), interpolation='none')
 
-            # Plot the target area
-            if self.target_area:
-                x, y = self.target_area.exterior.xy
-                ax.plot(x, y, 'y--', linewidth=2, label='Target Area')
-
             if all_polygons:
                 total_poly = unary_union(all_polygons)
                 if total_poly.geom_type == 'Polygon':
diff --git a/train copy.py b/train copy.py
deleted file mode 100644
index a2cae8d..0000000
--- a/train copy.py	
+++ /dev/null
@@ -1,136 +0,0 @@
-#!/usr/bin/env python3
-# train_optuna.py  â”€ Optuna ìµœì ê°’ìœ¼ë¡œ í•™ìŠµ & ëª¨ë“  ì½œë°± í¬í•¨
-# ----------------------------------------------------------
-import json, csv, gym, yaml, os
-from pathlib import Path
-from datetime import datetime
-
-from stable_baselines3 import PPO
-from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
-from stable_baselines3.common.callbacks import (
-    BaseCallback, CheckpointCallback, ProgressBarCallback
-)
-
-from run_manager import create_session   # â† ê¸°ì¡´ ì„¸ì…˜ ê´€ë¦¬ì
-
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-# 1. Optuna ê²°ê³¼ ë¡œë“œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´)
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-best_path = Path("best_reward_weights.json")
-if best_path.exists():
-    best = json.load(best_path.open())
-    print("âœ…  Optuna best íŒŒë¼ë¯¸í„° ë¡œë“œ:", best_path)
-else:
-    print("âš ï¸  best_reward_weights.json ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
-    best = dict(
-        w_area=1.0, w_alt=1.0, w_energy=1.0,
-        lr=3e-4, gamma=0.99, n_steps=2048,
-    )
-
-# í™˜ê²½ kwargs + PPO kwargs ë¡œ ë¶„ë¦¬
-ENV_KWARGS  = {k: best[k] for k in ("w_area", "w_alt", "w_energy")}
-PPO_KWARGS  = {k: best[k] for k in ("lr", "gamma", "n_steps")}
-PPO_KWARGS["learning_rate"] = PPO_KWARGS.pop("lr")
-PPO_KWARGS["verbose"]       = 1
-
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-# 2. ì„¸ì…˜ ë””ë ‰í„°ë¦¬ ìƒì„±
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-cfg = dict(
-    algo="PPO-optuna",
-    total_timesteps=2_000_000,
-    log_every_steps=20,
-    checkpoint_freq=10_000,
-)
-SESSION = create_session(cfg)
-LOG_DIR   = SESSION / "logs"
-PLOT_DIR  = SESSION / "plots"
-MODEL_DIR = SESSION / "models"
-TB_DIR    = SESSION / "tensorboard"
-
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-# 3. RewardPlotCallback  (ì›ë³¸ ë³´ìƒ*ë§Œ* ê¸°ë¡)
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-class RewardPlotCallback(BaseCallback):
-    def __init__(self, log_dir: Path, plot_every_episodes: int = 20, verbose: int = 0):
-        super().__init__(verbose)
-        self.log_dir   = Path(log_dir)
-        self.csv_path  = self.log_dir / "episode_returns.csv"
-        self.plot_path = self.log_dir / "reward_plot.png"
-        self.plot_every = plot_every_episodes
-
-    def _on_training_start(self):
-        self.n_envs = self.training_env.num_envs
-        self.ep_returns = [0.0] * self.n_envs
-        self.ep_lengths = [0]   * self.n_envs
-        self.total_episodes = 0
-        self.log_dir.mkdir(parents=True, exist_ok=True)
-        with open(self.csv_path, "w", newline="") as f:
-            csv.writer(f).writerow(["episode", "return", "length", "timesteps"])
-
-    def _on_step(self):
-        rewards = self.locals["rewards"]      # ì •ê·œí™” ë³´ìƒ
-        raw     = self.training_env.get_original_reward(rewards)
-        term    = self.locals.get("terminated", self.locals.get("dones"))
-        trunc   = self.locals.get("truncated",  [False]*len(term))
-
-        for i in range(self.n_envs):
-            self.ep_returns[i] += raw[i]
-            self.ep_lengths[i] += 1
-            if term[i]:                         # truncated ëŠ” ë¬´ì‹œ
-                self.total_episodes += 1
-                with open(self.csv_path, "a", newline="") as f:
-                    csv.writer(f).writerow(
-                        [self.total_episodes,
-                         float(self.ep_returns[i]),
-                         int(self.ep_lengths[i]),
-                         self.num_timesteps]
-                    )
-                self.ep_returns[i] = 0.0
-                self.ep_lengths[i] = 0
-
-        return True
-
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-# 4. Gym í™˜ê²½ ë“±ë¡ (ë”± í•œ ë²ˆ)
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-ENV_ID = "FlyDrone-v0-custom"
-if ENV_ID not in gym.envs.registry:
-    gym.register(
-        id=ENV_ID,
-        entry_point="fly_drone.envs.fly_drone_env:Fly_drone",
-        kwargs={**ENV_KWARGS, "log_dir": LOG_DIR, "plot_dir": PLOT_DIR},
-    )
-
-def make_env():
-    env = gym.make(ENV_ID)
-    env.settings(rend=False, train=True)
-    return env
-
-vec_env = DummyVecEnv([make_env])
-vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
-
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-# 5. ëª¨ë¸ ìƒì„± & ì½œë°±
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-model = PPO("MlpPolicy", vec_env,
-            tensorboard_log=str(TB_DIR),
-            **PPO_KWARGS)
-
-checkpoint_cb = CheckpointCallback(save_freq=cfg["checkpoint_freq"],
-                                   save_path=MODEL_DIR,
-                                   name_prefix="ckpt",
-                                   save_replay_buffer=False,
-                                   save_vecnormalize=True)
-rewardplot_cb = RewardPlotCallback(PLOT_DIR, plot_every_episodes=20)
-progress_cb   = ProgressBarCallback()
-
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-# 6. í•™ìŠµ
-# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-model.learn(total_timesteps=cfg["total_timesteps"],
-            callback=[checkpoint_cb, rewardplot_cb, progress_cb])
-
-model.save(MODEL_DIR / "final_model")
-vec_env.save(MODEL_DIR / "vec_normalize.pkl")
-print("ğŸ  Training finished:", SESSION)
diff --git a/train.py b/train.py
index d11ee74..6736132 100644
--- a/train.py
+++ b/train.py
@@ -4,7 +4,7 @@ from datetime import datetime
 from stable_baselines3 import PPO
 from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
 from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, ProgressBarCallback
-from fly_drone.envs import fly_drone_env
+from fly_drone.envs import fly_drone_env_og_ver
 import torch
 import csv
 import numpy as np