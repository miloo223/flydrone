#!/usr/bin/env python3
# train_optuna.py  â”€ Optuna ìµœì ê°’ìœ¼ë¡œ í•™ìŠµ & ëª¨ë“  ì½œë°± í¬í•¨
# ----------------------------------------------------------
import json, csv, gym, yaml, os
from pathlib import Path
from datetime import datetime

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import (
    BaseCallback, CheckpointCallback, ProgressBarCallback
)

from run_manager import create_session   # â† ê¸°ì¡´ ì„¸ì…˜ ê´€ë¦¬ì

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Optuna ê²°ê³¼ ë¡œë“œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
best_path = Path("best_reward_weights.json")
if best_path.exists():
    best = json.load(best_path.open())
    print("âœ…  Optuna best íŒŒë¼ë¯¸í„° ë¡œë“œ:", best_path)
else:
    print("âš ï¸  best_reward_weights.json ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
    best = dict(
        w_area=1.0, w_alt=1.0, w_energy=1.0,
        lr=3e-4, gamma=0.99, n_steps=2048,
    )

# í™˜ê²½ kwargs + PPO kwargs ë¡œ ë¶„ë¦¬
ENV_KWARGS  = {k: best[k] for k in ("w_area", "w_alt", "w_energy")}
PPO_KWARGS  = {k: best[k] for k in ("lr", "gamma", "n_steps")}
PPO_KWARGS["learning_rate"] = PPO_KWARGS.pop("lr")
PPO_KWARGS["verbose"]       = 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ì„¸ì…˜ ë””ë ‰í„°ë¦¬ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cfg = dict(
    algo="PPO-optuna",
    total_timesteps=2_000_000,
    log_every_steps=20,
    checkpoint_freq=10_000,
)
SESSION = create_session(cfg)
LOG_DIR   = SESSION / "logs"
PLOT_DIR  = SESSION / "plots"
MODEL_DIR = SESSION / "models"
TB_DIR    = SESSION / "tensorboard"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. RewardPlotCallback  (ì›ë³¸ ë³´ìƒ*ë§Œ* ê¸°ë¡)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RewardPlotCallback(BaseCallback):
    def __init__(self, log_dir: Path, plot_every_episodes: int = 20, verbose: int = 0):
        super().__init__(verbose)
        self.log_dir   = Path(log_dir)
        self.csv_path  = self.log_dir / "episode_returns.csv"
        self.plot_path = self.log_dir / "reward_plot.png"
        self.plot_every = plot_every_episodes

    def _on_training_start(self):
        self.n_envs = self.training_env.num_envs
        self.ep_returns = [0.0] * self.n_envs
        self.ep_lengths = [0]   * self.n_envs
        self.total_episodes = 0
        self.log_dir.mkdir(parents=True, exist_ok=True)
        with open(self.csv_path, "w", newline="") as f:
            csv.writer(f).writerow(["episode", "return", "length", "timesteps"])

    def _on_step(self):
        rewards = self.locals["rewards"]      # ì •ê·œí™” ë³´ìƒ
        raw     = self.training_env.get_original_reward(rewards)
        term    = self.locals.get("terminated", self.locals.get("dones"))
        trunc   = self.locals.get("truncated",  [False]*len(term))

        for i in range(self.n_envs):
            self.ep_returns[i] += raw[i]
            self.ep_lengths[i] += 1
            if term[i]:                         # truncated ëŠ” ë¬´ì‹œ
                self.total_episodes += 1
                with open(self.csv_path, "a", newline="") as f:
                    csv.writer(f).writerow(
                        [self.total_episodes,
                         float(self.ep_returns[i]),
                         int(self.ep_lengths[i]),
                         self.num_timesteps]
                    )
                self.ep_returns[i] = 0.0
                self.ep_lengths[i] = 0

        return True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Gym í™˜ê²½ ë“±ë¡ (ë”± í•œ ë²ˆ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENV_ID = "FlyDrone-v0-custom"
if ENV_ID not in gym.envs.registry:
    gym.register(
        id=ENV_ID,
        entry_point="fly_drone.envs.fly_drone_env:Fly_drone",
        kwargs={**ENV_KWARGS, "log_dir": LOG_DIR, "plot_dir": PLOT_DIR},
    )

def make_env():
    env = gym.make(ENV_ID)
    env.settings(rend=False, train=True)
    return env

vec_env = DummyVecEnv([make_env])
vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ëª¨ë¸ ìƒì„± & ì½œë°±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = PPO("MlpPolicy", vec_env,
            tensorboard_log=str(TB_DIR),
            **PPO_KWARGS)

checkpoint_cb = CheckpointCallback(save_freq=cfg["checkpoint_freq"],
                                   save_path=MODEL_DIR,
                                   name_prefix="ckpt",
                                   save_replay_buffer=False,
                                   save_vecnormalize=True)
rewardplot_cb = RewardPlotCallback(PLOT_DIR, plot_every_episodes=20)
progress_cb   = ProgressBarCallback()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. í•™ìŠµ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.learn(total_timesteps=cfg["total_timesteps"],
            callback=[checkpoint_cb, rewardplot_cb, progress_cb])

model.save(MODEL_DIR / "final_model")
vec_env.save(MODEL_DIR / "vec_normalize.pkl")
print("ğŸ  Training finished:", SESSION)
